\subsection{Implicit Bugs}
In addition to the execution of tasks, Hadoop also monitors these tasks and provides metrics reflecting their status for the purpose of rescheduling the killed ones.
In the Linux environment, TaskTrackers monitor tasks based on the following two conditions:  1) The memory usage of a specific task; 2) The total amount of memory usage of multiple tasks.
In the former situation, a task is killed and rescheduled by the TaskTracker if it fails either of the following two criteria:
i. Anytime when the current memory usage of a task exceeds two times of the value specified by {\bf mapred.cluster.max.map.memory.mb};
ii. The current memory usage of a task exceeds the value specified by {\bf mapred.cluster.max.map.memory.mb} for two consecutive periods of time (default is 5s). 
The first criteria considers the operation of fork(), which results in duplicated memory usages; the second criteria is applied to avoid the appearance of stragglers.
In the latter situation, tasks are killed by the TaskTracker if the following criteria is satisfied:
\begin{equation}
{\emph memInUsage} > {\emph maxMemAllowedForAllTasks} 
\end{equation}
where \emph {memInUsage} denotes the total memory in use of all the tasks scheduled by a specific TaskTracker and \emph {maxMemAllowedForAllTasks} denotes the total amount of memory allocated to it. The latter parameter can be calculated as following:
\begin{equation*}
\begin{aligned}
&{\emph maxMemAllowedForAllTasks} = \\
& \hspace {5pt} {\emph maxCurrMapTasks} \times {\emph mapSlotMemSizeOnTT} + \\
& \hspace {5pt} {\emph maxCurrReduceTasks} \times {\emph reduceSlotSizeMemOnTT}
\end{aligned}
\end{equation*}
In the above equation, \emph {maxCurrMapTasks} (specified by {\bf mapred.tasktracker.map.tasks.maximum} in the code) denotes the number of map slots allocated to that TaskTracker and \emph {mapSlotMemSizeOnTT}, which is specified by {\bf mapred.cluster.map.memory.mb}, denotes the memory size of a map slot. 
For reduce tasks, the situations are similar except that the parameters applied are different.
All tasks are killed by TaskTracker in the reverse order of scheduled time, \emph{i.e.}, recent tasks are killed until equation (1) fails. 
\par

\subsection{Related Works}
Even though Hadoop has its own monitor system on each component, including website on JobTracker to check the running status of each MapReduce job and website on Namenode to check the usage of Hadoop File System ( HDFS ), the information posted on these pages are always not enough for either naive or experienced users. For naive users, they can not get the detail description about why their job fails. For experienced users, they need to know more about the whole pipeline performance information that involves with a bunch of jobs. With the popularity of complex pipeline mapreduce jobs, based on the traditional monitoring frameworks for cluster nodes metrics, companies like LinkedIn, Hortonworks develop their own fine­grained monitor frameworks for Hadoop.
Traditional monitoring frameworks for large cluster nodes metrics can be divided to real­time monitor and batch monitor.
Ganglia [6], developed from the project named Millennium in UC Berkeley, is a scalable distributed monitor tool for high performance computing systems. It follows the master/slave infrastructure where on each node in cluster a background process called Gmond collects the performance metrics and then sends the data periodically to master node. Gmond is a multi­threaded daemon which monitors changes in host state, announce the metrics data in Unicasting or Multicasting way. Gmetad process runs on the master monitoring node (may not be the master of cluster), collects the metrics from distributed gmond processes and Ganglia has its own default PHP web front end to show the metrics with dashboards. Hadoop has support of integration with Ganglia. Nagios(Nagios Ain’t Goona Insist On Saintood) [7] is an open sourced monitoring system that has great freedom and flexibility. The core of Nagios is functional but small, but there are lots of plugins that can be used for specific requirement, and also developers can implement their own plugins. Plugins run on distributed nodes to monitor the status of the host and send metrics to the core. The advantages of Nagios is that it provides a bunch of alerts mechanism that can help administrators to be notified while there is a problem in the system.
Chukwa [8] is a subproject from Hadoop which is built directly on top of Hadoop, which is a distributed log analysis monitoring system. But different with Ganglia or Nagios, Chukwa is not real time monitoring system, but a batch log analysis system. With the ability of processing large data brought with Hadoop, Chukwa is a full stack solution for large scale log data, which includes data collection, storage, analysis and dashboard display. Chukwa also needs agent running on each node which monitors metrics statuson cluster nodes and Collector takes responsibility to collect data from distributed agents. After classification, sort, deduplicate and combination, those logs will be directly written to HDFS or HBase which can be used as the input source for MapReduce analysis jobs, then the result of the analysis will be showed through websites. Users can check the running status of MapReduce jobs including the run time, the resource, where the failure happens and where is the bottleneck of the whole pipeline which largely enriches the monitoring functionality.
InfluxDB [1] is a time series, metrics and analytics distributed database which is used as part of the monitoring system due to its specific design for storing metrics data and also support for abundant SQL query for analysis. Each node in cluster can insert metrics data into InfluxDB. Users need not care about the distributed message and data transferring as they just need to use database functions to update the metrics. It also provides well designed and easy to use website development tool, called Grafana, which greatly helps users integrate dashboards with analytics with less effort. SequenceIQ [2] is built directly on top of InfluxDB and Grafana.
Based on these monitoring frameworks mentioned above, IT companies build their own Hadoop monitor systems.
Cloudera Manager [3] is the industry’s first and most sophisticated management application for hadoop cluster. It is designed to make administration of enterprise data hub simple and straightforward, at any scale. It gives user a cluster­wide, real­time view of nodes/jobs running status, and incorporates a full range of reporting and diagnostic tools to help user optimize performance.
Ambri [4] developed by Hortonworks which can be said an open source version of Cloudera Manager which can be used to monitor the status of hadoop jobs. Ambri use Ganglia to collect metrics, and use Nagios to support the alert system, users can check the status of Hadoop core and also HBase, Hive. For better integration with existed administration tools, Ambri also expose the monitoring metrics through RESTful API. White Elephant [5], developed by LinkedIn, is an open sourced Hadoop log aggregator and dashboards which enables visualization of Hadoop cluster utilization across users. A task runs on JobTracker which periodically collects log data in cluster node, then a sequence of MapReduce jobs to compute aggregate statistics, lastly a viewer app displays the metrics with dashboard. It is another implementation of Chukwa.
The difference between our system and Chukwa and White Elephant is that we use real­time metric collected by Ganglia, instead of the logs in each node. Unlike Cloudera Manager, Ambri, and White Elephant providing a more general monitoring system, our system is focusing on memory usage probles. Our system provides a more fine­grained monitoring and event detection for memory.


