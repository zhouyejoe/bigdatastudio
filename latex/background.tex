\subsection{Implicit Bugs}
In addition to the execution of tasks, Hadoop also monitors these tasks and provides metrics reflecting their status for the purpose of rescheduling the killed ones.
In the Linux environment, TaskTrackers monitor tasks based on the following two conditions:  1) The memory usage of a specific task; 2) The total amount of memory usage of multiple tasks.
In the former situation, a task is killed and rescheduled by the TaskTracker if it fails either of the following two criteria:
i. Anytime when the current memory usage of a task exceeds two times of the value specified by {\bf mapred.cluster.max.map.memory.mb};
ii. The current memory usage of a task exceeds the value specified by {\bf mapred.cluster.max.map.memory.mb} for two consecutive periods of time (default is 5s). 
The first criteria considers the operation of fork(), which results in duplicated memory usages; the second criteria is applied to avoid the appearance of stragglers.
In the latter situation, tasks are killed by the TaskTracker if the following criteria is satisfied:
\begin{equation}
{\emph memInUsage} > {\emph maxMemAllowedForAllTasks} 
\end{equation}
where \emph {memInUsage} denotes the total memory in use of all the tasks scheduled by a specific TaskTracker and \emph {maxMemAllowedForAllTasks} denotes the total amount of memory allocated to it. The latter parameter can be calculated as following:
\begin{equation*}
\begin{aligned}
&{\emph maxMemAllowedForAllTasks} = \\
& \hspace {5pt} {\emph maxCurrMapTasks} \times {\emph mapSlotMemSizeOnTT} + \\
& \hspace {5pt} {\emph maxCurrReduceTasks} \times {\emph reduceSlotSizeMemOnTT}
\end{aligned}
\end{equation*}
In the above equation, \emph {maxCurrMapTasks} (specified by {\bf mapred.tasktracker.map.tasks.maximum} in the code) denotes the number of map slots allocated to that TaskTracker and \emph {mapSlotMemSizeOnTT}, which is specified by {\bf mapred.cluster.map.memory.mb}, denotes the memory size of a map slot. 
For reduce tasks, the situations are similar except that the parameters applied are different.
All tasks are killed by TaskTracker in the reverse order of scheduled time, \emph{i.e.}, recent tasks are killed until equation (1) fails. 
\par

\subsection{Related Work}
Even though Hadoop has its own monitoring system on each component, including website on JobTracker to check the running status of each MapReduce job and website on Namenode to check the usage of Hadoop File System ( HDFS ), the information posted on these pages is always not enough for either naive or experienced users.
For naive users, they merely get the log description showing the failed status of their jobs without specific reasons; For experienced users who need to know more about the whole pipeline performance information involved with a bunch of jobs, the default monitoring system is not enough either.
With the popularity of complex pipeline MapReduce jobs, companies like LinkedIn, Hortonworks develop their own fine-grained monitor frameworks for Hadoop based on traditional monitoring frameworks for cluster nodes metrics.\par
Ganglia\cite{massie2004ganglia}, developed from the project named Millennium in UC Berkeley, is a scalable distributed monitoring tool for high performance computing systems.
Ganglia follows the master/slave infrastructure, running a Ganglia monitoring daemon (Gmond) on each node in the cluster that needs to be monitored.
Gmond collects metric information on this node such as metrics of CPU,memory, etc., and sends it to Ganglia Meta Daemon (Gmetad) running on the master monitoring node of the cluster via XML over TCP periodically.
The Gmetad process runs on the master monitoring node (may not be the master of a cluster) collecting the metrics from distributed Gmond processes and Ganglia has its own default PHP web front end to show these metrics with dashboards.
%Gmond is a multi-threaded daemon which monitors changes in host state, announce the metrics data in Unicasting or Multicasting way. Hadoop has support of integration with Ganglia


Nagios (Nagios Ain’t Goona Insist On Saintood) \cite{josephsen2007building} is an open-source monitoring system that has great freedom and flexibility. 
The core of Nagios is concise, but it can be extended with specific plug-ins for different requirements, including plug-ins implemented by the developers.
These plug-ins run on distributed nodes to monitor the their status and send metrics to the core. 
The advantages of Nagios is that it provides a bunch of alerts mechanism that can notify the administrator when there is a problem in the system.
\par
Chukwa\cite{boulon2008chukwa} is a subproject of Hadoop, which is a distributed log analysis monitoring system. 
Different with Ganglia or Nagios, which are real-time monitoring systems, Chukwa is a batch log analysis system with the ability of processing large data brought by Hadoop.
Chukwa provides a full stack solution for large scale log data including data collection, storage, analysis and dashboard display.
Chukwa also needs a agent running on each node which monitors metrics status on cluster nodes and Collector takes responsibility to collect data from distributed agents. After classification, sort, deduplicate and combination, those logs will be directly written to HDFS or HBase which can be used as the input source for MapReduce analysis jobs, then the result of the analysis will be showed through websites. Users can check the running status of MapReduce jobs including the run time, the resource, where the failure happens and where is the bottleneck of the whole pipeline which largely enriches the monitoring functionality.


InfluxDB [1] is a time series, metrics and analytics distributed database which is used as part of the monitoring system due to its specific design for storing metrics data and also support for abundant SQL query for analysis. Each node in cluster can insert metrics data into InfluxDB. Users need not care about the distributed message and data transferring as they just need to use database functions to update the metrics. It also provides well designed and easy to use website development tool, called Grafana, which greatly helps users integrate dashboards with analytics with less effort. SequenceIQ [2] is built directly on top of InfluxDB and Grafana.
Based on these monitoring frameworks mentioned above, IT companies build their own Hadoop monitor systems.
Cloudera Manager [3] is the industry’s first and most sophisticated management application for hadoop cluster. It is designed to make administration of enterprise data hub simple and straightforward, at any scale. It gives user a cluster­wide, real­time view of nodes/jobs running status, and incorporates a full range of reporting and diagnostic tools to help user optimize performance.
Ambri [4] developed by Hortonworks which can be said an open source version of Cloudera Manager which can be used to monitor the status of hadoop jobs. Ambri use Ganglia to collect metrics, and use Nagios to support the alert system, users can check the status of Hadoop core and also HBase, Hive. For better integration with existed administration tools, Ambri also expose the monitoring metrics through RESTful API. White Elephant [5], developed by LinkedIn, is an open sourced Hadoop log aggregator and dashboards which enables visualization of Hadoop cluster utilization across users. A task runs on JobTracker which periodically collects log data in cluster node, then a sequence of MapReduce jobs to compute aggregate statistics, lastly a viewer app displays the metrics with dashboard. It is another implementation of Chukwa.
The difference between our system and Chukwa and White Elephant is that we use real­time metric collected by Ganglia, instead of the logs in each node. Unlike Cloudera Manager, Ambri, and White Elephant providing a more general monitoring system, our system is focusing on memory usage probles. Our system provides a more fine­grained monitoring and event detection for memory.


