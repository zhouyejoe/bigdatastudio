
The publication of Google MapReduce and its open-source version, Hadoop, simplify the large-scale data processing procedures and improve the computation efficiency by scheduling a job into multiple tasks running in parallel. 
Tasks with memory consumption exceeding the limit of operating systems are killed and restarted, which might result in both time and memory inefficiency, and this is common for users running their MapRedue programs with the default parameter settings.
To improve this situation, we present a memory failures debugger based on Ganglia to monitor the memory consumption behaviors of Hadoop version 1.2.1 jobs through JMX in a real-time manner. We build a set of MapReduce benchmark jobs to mimic some forms of jobs that might result in exceeding memory consumption and the reschedule of specific tasks. We also develop a parameter recommender, Memtirc, to assist users with tuning of parameters required to avoid memory failures.

