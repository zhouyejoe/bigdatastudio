\begin{figure*}[ht]
  \centering
    \includegraphics[width=4.0in]{image/architecture.png}
    \caption{System Architecture}
    \label{ref:architecture}
\end{figure*}

Figure \ref{ref:architecture} shows our system design. It contains three main components: \textbf{Ganglia}, \textbf{Hadoop metric system}, and \textbf{Memmetric}. We used Ganglia 3.6.0 as the infrastructure to gather the  Hadoop cluster metrics in real-time. Hadoop cluster periodically reports its information to Ganglia daemon. After Ganglia collecting the metric information, they will be organized and visualized to users by our frontend system: Memmetric.

\subsection{Ganglia}

Ganglia deploys daemons on each node, called as gmond. Gmond in a worker node is responsible to collect monitoring statistics of a machine (e.g., CPU load, memory usage, disk load), and report those metrics to the gmond on the server node. 

Other programs also can send its metrics to the gmond in the same machine, and then the metrics will be aggregated by Ganglia system. In our system, we set Hadoop cluster to send Java Virtual Machine (JVM) metric, which includes task level memory usage information to gmond.

Gmetad polls the metrics from the gmond on the server node, and stores the information into round robin database (rrd) periodically. Besides, Ganglia periodically compass old metric values in rrd, which can prevent rrd waste too much disk space to store less used old metric values.

Ganglia provides a web interface called ganglia-web, which can plots the metrics in rrds in the form of charts or json. 

We used ganglia-web as API server. Our frontend system queries ganglia-web for the information we needed, organize them, and visualize those information. 

\subsubsection{Ganglia Filter}

\begin{figure}[ht]
  \centering
    \includegraphics[width=3.0in]{image/ganglia_finished_tasks.png}
    \caption{Ganglia does not set the value of finished tasks as zero}
    \label{ref:gangliah_bug}
\end{figure}

We found that Ganglia keeps to report finished task's metric as the last value it received because Ganglia does not discard received metrics in default. For example, Figure \ref{ref:gangliah_bug} shows the heap memory usage of tasks in a Hadoop job. There should be only four map tasks running after 22:35 because we set two map task slots for a machine, but there are six lines of memory usage. The task(jvm.metrics. jvm\_201505072223\_0001\_m\_-296954387. memHeapUsedM) has finished at 22:29, but the metric value is not set to zero.

It makes the reported memory usage in a machine incredible large because the finished task memory usages are all counted into the sum of memory usage, which makes the sum unreasonably high. 

One solution is to set the discard time d\_max. When a metric is not reported to Ganglia over d\_max time, Ganglia will assume the metric is dead and discard it. However, the value of discarded metrics is not set as zero, but it becomes invisible from Ganglia web-interface, which means that our frontend cannot plot any metrics before d\_max and cannot provide history memory usage information to users. It becomes a dilemma: if d\_max is set to a small number, the finished task metric will be discarded and it will not be counted into the sum of memory usage, but it becomes invisible from the frontend. If d\_max is a large number, the finished task information can retain for longer time, but its value is not set to zero and confuses the sum of total memory usage.

Instead of modifying d\_max, we added a filter inside the Ganglia API server to clean the metrics of finished tasks. If one metric is continual reported as same value over a tolerated period $T$ from time t\_end, the filter will assume the task finishes at t\_end and set the value of the metric after t\_end. 

\subsection{Hadoop Metric System}

Hadoop 1.2.1 already provides metric system to gather different metrics source (e.g., JVM metrics, Remote Procedure Call metrics) in Hadoop cluster and send them to the assigned destination (e.g., files or Ganglia daemon), shown in Figure \ref{fig:metric2}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/ganglia32}
  \caption{Metric System in Hadoop (Metric2)}
  \label{fig:metric2}
\end{figure}

In our system, we connected "JVM metrics" to metric system, and assigned them sending to Ganglia by GangliaSink. Therefore, Ganglia can receive task level memory usage information from Hadoop metric system.

We found that the default connector between metric system and Ganglia (GangliaSink31.java) behaves unexpectedly; therefore, we revised it to GangliaSink32 to monitor task level information, which is discussed in next section.

\subsubsection{Revised GangliaSink: GangliaSink32}

Each JVM in Hadoop cluster periodically reports it memory usage information into metric system as a key-value pair. The key format is jvm.metric.\{tag\}.heapMemoryUsedM. For example, jvm.metric.jvm\_20140405\_1\_m\_1.heapMemoryUsedM means the heap memory usage of the map task for Hadoop job 20140405\_1. 

However, the original connector between metric system and Ganglia, GangliaSink31.java, will discard the tag of the reported metric key. For example, jvm.metric.jvm\_20140405\_0001\_m\_1.heapMemoryUsedM will become jvm.metric.heapMemoryUsedM. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/ganglia31_flow.png}
  \caption{GangliaSink31 removes the tag in the key of metric and cause ambiguity.}
\end{figure}

Removing the tag from the reported metric results in that the system cannot discriminate metrics from different running tasks. Therefore, the meaning of jvm.metric.heapMemoryUsedM becomes hard to explain, because we cannot know which target's memory usage does  jvm.metric.heapMemoryUsedM stands for. Also, the system cannot aggregate all task memory usage into a machine aspect memory usage, because they are all mixed in a metric. 

To solve this problem, we developed GangliaSink32.java to replace GangliaSink31.java. GangliaSink32.java can retain the tag of the metric key. Therefore, Ganglia can separate the memory usage information among different processes. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{image/ganglia32_flow.png}
  \caption{GangliaSink32 retains the tag in the key of metric.}
\end{figure}

\subsection{Frontend: Memmetric}
We designed our frontend system (Memmetric) in two parts: \textbf{real-time monitor} and \textbf{historical monitor}. Real-time monitor  presents users the current status of the clusters. Historical monitor records anomalous data points to hint users debug their cluster.

\subsubsection{Real-time Monitor}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/real-time-monitor.png}
  \caption{Real-time Monitor}
\end{figure}

Real-time monitor shows per-machine view of task heap memory usage, the number of running map tasks and reduce tasks. Each line in "Memory Usage" block shows the total heap memory usage in a machine. Also, lines in "Map Running Tasks" and "Reduce Running Tasks" show the number of map an reduce tasks in different machines.

Hadoop cluster reports the memory usage for each task to Ganglia. However, plotting the memory usage of all tasks individually on the same chart can be hard to understand, because a Hadoop job may spawn hundreds of tasks. Therefore, Memmetric aggregates all per-task heap memory usage (jvm.metrics.jvm\_\{process\_id and task type\}.memHeapUsedM) on a machine into a per-machine heap memory usage line. Also, Memmetric shows the number of running map tasks and reduce tasks. Real-time monitor provides a clue whether there are tasks killed due to over memory usage. If there is a sharp drop of heap memory usage on machine, it is possible that some tasks use too many memory to be killed by JVM or operating system. 

\subsubsection{Historical Monitor}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/historical-monitor.png}
  \caption{Historical Monitor and Parameter Settings}
\end{figure}

Historical monitor presents the odd data points in each machine for users to revise their cluster parameter settings. Memmetric records the data points of largest task heap memory usage in each machine, because those points are the possible moments where a task violates the memory upper bound restriction and results in failure. For example, if Hadoop cluster sets "mapred.child.java.opt" as "-Xmx200m", which means that  the JVM child of a task at most allocate 200MB memory. If the historical monitor shows that some task has used more than 200MB, it is highly possible that the task will be killed, and the user should consider to modify the parameter to enlarge the maximum heap memory usage.

Besides, Memmetric crawls the configure files in Hadoop directory (i.e. "hadoop/conf") to fetch the parameter settings, and shows them nearby historical monitor. Users can revise their cluster's settings by comparing historical monitor and parameter settings. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/overview.png}
  \caption{Memmetric Overview}
\end{figure}

