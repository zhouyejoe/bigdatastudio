\begin{figure*}[ht]
  \centering
    \includegraphics[width=4.0in]{image/architecture.png}
    \caption{System Architecture}
    \label{ref:architecture}
\end{figure*}

Figure \ref{ref:architecture} shows our system design. It contains three main components: \textbf{Ganglia}, \textbf{Hadoop metric system}, and \textbf{Memmetric}. We used Ganglia 3.6.0 as the infrastructure to gather the  Hadoop cluster metrics in real-time. Each node in the Hadoop cluster periodically reports its information to Ganglia daemon. After Ganglia collecting the metric information, it will be organized and visualized to users through the frontend of Memmetric.

\subsection{Ganglia}

Ganglia deploys a daemon on each node in the cluster called Gmond. Gmond in a worker node is responsible for collecting monitoring statistics of a machine (\emph{e.g.}, CPU load, memory usage, disk load), and reporting these metrics to the Gmond on the master node. 

Other programs can also send their metrics to the Gmond in the same machine, and then the metrics will be aggregated by Ganglia system. In Memmetric, we deploy Hadoop cluster to send JVM metric which includes task level memory usage information to Gmond.

Gmetad polls the metrics from the Gmond on the master node, and stores the information into round robin database (rrd) periodically. In addition, Ganglia periodically compasses old metric values in rrd, thus avoiding rrd wasting too much disk space to store less-used or antiquated metrics.

Ganglia provides a web interface called ganglia-web, which can plots the metrics in rrds in the form of charts or json. Memmetric queries ganglia-web for the information we needed, organizes them, and visualizes these information. 

\subsubsection{Ganglia Filter}

\begin{figure}[ht]
  \centering
    \includegraphics[width=3.0in]{image/ganglia_finished_tasks.png}
    \caption{Ganglia does not set the value of finished tasks as zero}
    \label{ref:gangliah_bug}
\end{figure}

During the process of the deployment of Memmetric, we find that Ganglia keeps reporting finished tasks' metrics as the last values it receives because that Ganglia does not discard received metrics automatically. For example, Figure \ref{ref:gangliah_bug} shows the heap memory usage of tasks in a regular Hadoop job. Presumably, there should be only four map tasks running after 22:35 because we set two map task slots for a machine; however, it turns out that there are six lines of memory usage, one of them, jvm.metrics.jvm\_201505072223\_0001\_m\_-296954387.memHeapUsedM displayed in light blue line finishes at 22:29, but the metric value remains.

This behavior in Ganglia renders the reported memory usage in a machine unreasonably large because the finished task memory usages are all counted into the sum of total memory usage. One solution to this problem is to tune the parameter "delete time" d\_max: When metric is not sent to Ganglia over d\_max time, Ganglia will assume the metric is dead and discard it. However, the value of discarded metrics is not set as zero, but it becomes invisible from Ganglia web-interface, which means that our frontend cannot plot any metrics before d\_max and cannot provide history memory usage information to users. It becomes a dilemma: if d\_max is set to a small number, the finished task metric will be discarded and it will not be counted into the sum of memory usage, but it becomes invisible from the frontend. If d\_max is a large number, the finished task information can retain for longer time, but its value is not set to zero and confuses the sum of total memory usage.

Instead of modifying d\_max, we added a filter inside the Ganglia API server to clean the metrics of finished tasks. If one metric is continual reported as same value over a tolerated period $T$ from time t\_end, the filter will assume the task finishes at t\_end and set the value of the metric after t\_end. 

\subsection{Hadoop Metric System}

Hadoop 1.2.1 already provides metric system to gather different metrics source (e.g., JVM metrics, Remote Procedure Call metrics) in Hadoop cluster and send them to the assigned destination (e.g., files or Ganglia daemon), shown in Figure \ref{fig:metric2}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/ganglia32}
  \caption{Metric System in Hadoop (Metric2)}
  \label{fig:metric2}
\end{figure}

In our system, we connected "JVM metrics" to metric system, and assigned them sending to Ganglia by GangliaSink. Therefore, Ganglia can receive task level memory usage information from Hadoop metric system.

We found that the default connector between metric system and Ganglia (GangliaSink31.java) behaves unexpectedly; therefore, we revised it to GangliaSink32 to monitor task level information, which is discussed in next section.

\subsubsection{Revised GangliaSink: GangliaSink32}

Each JVM in Hadoop cluster periodically reports it memory usage information into metric system as a key-value pair. The key format is jvm.metric.\{tag\}.heapMemoryUsedM. For example, jvm.metric.jvm\_20140405\_1\_m\_1.heapMemoryUsedM means the heap memory usage of the map task for Hadoop job 20140405\_1. 

However, the original connector between metric system and Ganglia, GangliaSink31.java, will discard the tag of the reported metric key. For example, jvm.metric.jvm\_20140405\_0001\_m\_1.heapMemoryUsedM will become jvm.metric.heapMemoryUsedM. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/ganglia31_flow.png}
  \caption{GangliaSink31 removes the tag in the key of metric and cause ambiguity.}
\end{figure}

Removing the tag results in that the system cannot discriminate metrics from different running tasks. Therefore, the meaning of jvm.metric.heapMemoryUsedM becomes hard to explain, because we cannot know which target's memory usage does  jvm.metric.heapMemoryUsedM stands for. Also, the system cannot aggregate all task memory usage into a machine aspect memory usage, because they are all mixed in a metric. 

To solve this problem, we developed GangliaSink32.java to replace GangliaSink31.java. GangliaSink32.java can retain the tag of the metric key. Therefore, Ganglia can separate the memory usage information among different processes. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{image/ganglia32_flow.png}
  \caption{GangliaSink32 retains the tag in the key of metric.}
\end{figure}

\subsubsection{Reporting Frequency}

Ganglia can configure the reporting frequency of gmond.  In default, gmond reports in 10 seconds. However, some small map tasks can run in less than 10 seconds, which cannot be observed because it ends before gmond starts to collect memory usage metrics. 
We assigned reporting frequency as 2 seconds to detect fast tasks.

\subsection{Frontend: Memmetric}
We designed our frontend system (Memmetric) in two parts: \textbf{real-time monitor} and \textbf{historical monitor}. Real-time monitor  presents users the current status of the clusters. Historical monitor records anomalous data points to hint users debug their cluster.

\subsubsection{Real-time Monitor}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/real-time-monitor.png}
  \caption{Real-time Monitor}
\end{figure}

Real-time monitor shows per-machine view of task heap memory usage, the number of running map tasks and reduce tasks. Each line in "Memory Usage" block shows the total heap memory usage in a machine. Also, lines in "Map Running Tasks" and "Reduce Running Tasks" show the number of map an reduce tasks in different machines.

Hadoop cluster reports the memory usage for each task to Ganglia. However, plotting the memory usage of all tasks individually on the same chart can be hard to understand, because a Hadoop job may spawn hundreds of tasks. Therefore, Memmetric aggregates all per-task heap memory usage (jvm.metrics.jvm\_\{process\_id and task type\}.memHeapUsedM) on a machine into a per-machine heap memory usage line. Also, Memmetric shows the number of running map tasks and reduce tasks. Real-time monitor provides a clue whether there are tasks killed due to over memory usage. If there is a sharp drop of heap memory usage on machine, it is possible that some tasks use too many memory to be killed by JVM or operating system. 

\subsubsection{Historical Monitor}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/historical-monitor.png}
  \caption{Historical Monitor and Parameter Settings}
\end{figure}

Historical monitor presents the odd data points in each machine for users to revise their cluster parameter settings. Memmetric records the data points of largest task heap memory usage in each machine, because those points are the possible moments where a task violates the memory upper bound restriction and results in failure. For example, if Hadoop cluster sets "mapred.child.java.opt" as "-Xmx200m", which means that  the JVM child of a task at most allocate 200MB memory. If the historical monitor shows that some task has used more than 200MB, it is highly possible that the task will be killed, and the user should consider to modify the parameter to enlarge the maximum heap memory usage.

Besides, Memmetric crawls the configure files in Hadoop directory (i.e. "hadoop/conf") to fetch the parameter settings, and shows them nearby historical monitor. Users can revise their cluster's settings by comparing historical monitor and parameter settings. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{image/overview.png}
  \caption{Memmetric Overview}
\end{figure}

